{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Scratch Artificial Neural Network",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T00:11:37.031927Z",
     "start_time": "2025-03-10T00:11:37.028769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import libraries\n",
    "# External libraries\n",
    "import torch\n",
    "\n",
    "# Python built-in libraries\n",
    "from functools import lru_cache\n",
    "from enum import Enum"
   ],
   "id": "7df9140457caed5e",
   "outputs": [],
   "execution_count": 1549
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T00:11:37.047467Z",
     "start_time": "2025-03-10T00:11:37.043843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Disable __grad based on assistant direction\n",
    "torch.set_grad_enabled(False)"
   ],
   "id": "5ccd2101c23e2da9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x19290a76a30>"
      ]
     },
     "execution_count": 1550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1550
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Classes for Artificial Neural Network",
   "id": "4be3dbc77bf6c823"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T00:11:37.068206Z",
     "start_time": "2025-03-10T00:11:37.062478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ActivationFunction:\n",
    "    \"\"\"\n",
    "    ActivationFunction Class.\n",
    "    \n",
    "    This class is used to define the activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=None)\n",
    "    def linear(x):\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=None)\n",
    "    def relu(x):\n",
    "        return torch.relu(x)\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=None)\n",
    "    def sigmoid(x):\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=None)\n",
    "    def tanh(x):\n",
    "        return torch.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=None)\n",
    "    def softmax(x):\n",
    "        return torch.softmax(x, dim=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_linear(_):\n",
    "        return 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_relu(x):\n",
    "        return torch.where(x > 0, 1, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_sigmoid(x):\n",
    "        return torch.sigmoid(x) * (1 - torch.sigmoid(x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_tanh(x):\n",
    "        return (2 / torch.exp(x) - torch.exp(-x)) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def derivative_softmax(x):\n",
    "        s = torch.softmax(x, dim=0).reshape(-1, 1)\n",
    "        return torch.diagflat(s) - torch.mm(s, s.T)"
   ],
   "id": "3102da3f17a7e376",
   "outputs": [],
   "execution_count": 1551
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T00:11:37.086437Z",
     "start_time": "2025-03-10T00:11:37.082801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LossFunction:\n",
    "    \"\"\"\n",
    "    LossFunction Class.\n",
    "    \n",
    "    This class is used to define the loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_squared_error(y_pred, y_true):\n",
    "        return torch.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def binary_cross_entropy(y_pred, y_true):\n",
    "        return -torch.mean(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "\n",
    "    @staticmethod\n",
    "    def categorical_cross_entropy(y_pred, y_true):\n",
    "        return -torch.sum(y_true * torch.log(y_pred))"
   ],
   "id": "f2d2791826ce2995",
   "outputs": [],
   "execution_count": 1552
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T00:11:37.103589Z",
     "start_time": "2025-03-10T00:11:37.100466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class InitializerType(Enum):\n",
    "    \"\"\"\n",
    "    InitializerType Enum.\n",
    "    \n",
    "    This enum is used to define the type of weight initialization.\n",
    "    Attributes:\n",
    "        ZERO: Zero initialization\n",
    "        RANDOM_DIST_UNIFORM: Random distribution uniform initialization\n",
    "        RANDOM_DIST_NORMAL: Random distribution normal initialization\n",
    "    \"\"\"\n",
    "    ZERO = 0\n",
    "    RANDOM_DIST_UNIFORM = 1\n",
    "    RANDOM_DIST_NORMAL = 2"
   ],
   "id": "829a7d3f4b16addc",
   "outputs": [],
   "execution_count": 1553
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T00:11:37.122295Z",
     "start_time": "2025-03-10T00:11:37.118093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Initializer:\n",
    "    \"\"\"\n",
    "    Initializer Class.\n",
    "    \n",
    "    This class is used to initialize weights and biases.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def init_weights(weight_init : InitializerType, param_1, param_2, size: int, seeds):\n",
    "        \"\"\"\n",
    "        Initialize weights\n",
    "        :param weight_init: Type of weight initialization\n",
    "        :param param_1: Lower bound or mean\n",
    "        :param param_2: Upper bound or standard deviation\n",
    "        :param size: Size of the weight\n",
    "        :param seeds: Random seed\n",
    "        :return: Initialized weights\n",
    "        \"\"\"\n",
    "        if weight_init == InitializerType.ZERO:\n",
    "            return Initializer.zero_init(size)\n",
    "        elif weight_init == InitializerType.RANDOM_DIST_UNIFORM:\n",
    "            return Initializer.random_dist_uniform(size, param_1, param_2, seeds)\n",
    "        elif weight_init == InitializerType.RANDOM_DIST_NORMAL:\n",
    "            return Initializer.random_dist_normal(size, param_1, param_2, seeds)\n",
    "\n",
    "    @staticmethod\n",
    "    def zero_init(size):\n",
    "        return torch.zeros(size)\n",
    "\n",
    "    @staticmethod\n",
    "    def random_dist_uniform(size, lower_bound, upper_bound, seeds):\n",
    "        torch.manual_seed(seeds)\n",
    "        return torch.rand(size) * (upper_bound - lower_bound) + lower_bound\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_dist_normal(size, mean, std, seeds):\n",
    "        torch.manual_seed(seeds)\n",
    "        return torch.randn(size) * std + mean"
   ],
   "id": "1013ba92464b7584",
   "outputs": [],
   "execution_count": 1554
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T00:11:37.142665Z",
     "start_time": "2025-03-10T00:11:37.137200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Layer Class.\n",
    "    \n",
    "    Consist of many neurons of Type Layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight_init : InitializerType, bias_init : InitializerType, input_size, output_size, param_1, param_2, seeds = 0, activation = ActivationFunction.linear, layer_name = None):\n",
    "        self.layer_name = layer_name\n",
    "        self.neurons = [Neuron(weight_init, bias_init, input_size, param_1, param_2, seeds) for _ in range(output_size)]\n",
    "        self.activation_func = activation\n",
    "        self.sum = None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.sum = torch.stack([neuron.forward(x) for neuron in self.neurons])[0]\n",
    "        self.output = self.activation_func(self.sum)\n",
    "        return self.output\n",
    "    \n",
    "    def weight_update(self, lr, grad):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.weight_update(lr, grad)\n",
    "    \n",
    "    def bias_update(self, lr, grad):\n",
    "        for neuron, grad in zip(self.neurons, grad):\n",
    "            neuron.bias_update(lr, grad)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"Layer Name: {self.layer_name}\\nNeurons: {len(self.neurons)}\\n\""
   ],
   "id": "e47d0abebcf3846d",
   "outputs": [],
   "execution_count": 1555
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-10T00:11:37.161949Z",
     "start_time": "2025-03-10T00:11:37.157279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Neuron:\n",
    "    \"\"\"\n",
    "    Neuron Class.\n",
    "    \n",
    "    Consist of weights and bias for every input feature.\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_init : InitializerType, bias_init : InitializerType, input_size, param_1, param_2, seeds = 0):\n",
    "        self.weights = Initializer.init_weights(weight_init, param_1, param_2, input_size, seeds)\n",
    "        self.bias = Initializer.init_weights(bias_init, param_1, param_2, input_size, seeds)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = 0\n",
    "        for weight, x in zip(self.weights, x):\n",
    "            res += weight * x\n",
    "        return res + self.bias\n",
    "    \n",
    "    def weight_update(self, lr, grad):\n",
    "        self.weights -= lr * grad\n",
    "        \n",
    "    def bias_update(self, lr, grad):\n",
    "        self.bias -= lr * grad"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1556
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T00:11:37.180206Z",
     "start_time": "2025-03-10T00:11:37.176050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class OutputLayer(Layer):\n",
    "    \"\"\"\n",
    "    OutputLayer Class.\n",
    "    \n",
    "    Consist of many neurons of Output Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_init : InitializerType, bias_init : InitializerType, input_size, output_size, param_1, param_2, seeds = 0, activation = ActivationFunction.linear, layer_name = None):\n",
    "        super().__init__(weight_init, bias_init, input_size, output_size, param_1, param_2, seeds, activation, layer_name)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.sum = torch.stack([neuron.forward(x) for neuron in self.neurons])[0]\n",
    "        self.output = ActivationFunction.softmax(self.sum)\n",
    "        return self.output"
   ],
   "id": "808b47ccd483552c",
   "outputs": [],
   "execution_count": 1557
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T00:11:37.202679Z",
     "start_time": "2025-03-10T00:11:37.194773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ArtificialNeuralNetwork:\n",
    "    \"\"\"\n",
    "    ArtificialNeuralNetwork Class.\n",
    "    \n",
    "    Consist of many layers of Type Layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_layers, hidden_size, weight_init, bias_init, param_1, param_2, seeds = 0, activation = ActivationFunction.linear):\n",
    "        self.layers = []\n",
    "        self.layers.append(Layer(weight_init, bias_init, input_size, hidden_size, param_1, param_2, seeds, activation, \"Input Layer\"))\n",
    "        for i in range(hidden_layers):\n",
    "            self.layers.append(Layer(weight_init, bias_init, hidden_size, hidden_size, param_1, param_2, seeds, activation, f\"Hidden Layer {i}\"))\n",
    "        self.layers.append(OutputLayer(weight_init, bias_init, hidden_size, output_size, param_1, param_2, seeds, activation, \"Output Layer\"))\n",
    "        self.derivative_activation = None\n",
    "        if activation == ActivationFunction.sigmoid:\n",
    "            self.derivative_activation = ActivationFunction.derivative_sigmoid\n",
    "        elif activation == ActivationFunction.linear:\n",
    "            self.derivative_activation = ActivationFunction.derivative_linear\n",
    "        elif activation == ActivationFunction.relu:\n",
    "            self.derivative_activation = ActivationFunction.derivative_relu\n",
    "        elif activation == ActivationFunction.tanh:\n",
    "            self.derivative_activation = ActivationFunction.derivative_tanh\n",
    "        elif activation == ActivationFunction.softmax:\n",
    "            self.derivative_activation = ActivationFunction.derivative_softmax\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            print(layer)\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, lr, output, target):\n",
    "        print(target, output)\n",
    "        for layer in reversed(self.layers):\n",
    "            error_node = output * (1 - output) * (output - target)\n",
    "            delta_w = lr * error_node * output\n",
    "            delta_b = lr * error_node\n",
    "            layer.weight_update(lr, delta_w)\n",
    "            layer.bias_update(lr, delta_b)\n",
    "            output = error_node\n",
    "        return output\n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "    def train(self, x, y, lr, epochs, loss_func, verbose = False):\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for x_input, y_input in zip(x, y):\n",
    "                y_pred = self.forward(x_input)\n",
    "                loss = loss_func(y_pred, y_input)\n",
    "                self.backward(lr, y_pred, y_input)\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch} - Loss: {loss}\")"
   ],
   "id": "186cab437e129e46",
   "outputs": [],
   "execution_count": 1558
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pipeline for testing MNIST dataset",
   "id": "6c34fd47c03a7a52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T00:11:37.220934Z",
     "start_time": "2025-03-10T00:11:37.217836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Const variables\n",
    "input_size = 784\n",
    "hidden_layers = 1\n",
    "hidden_size = 41000\n",
    "output_size = 10\n",
    "learning_rate = 0.01"
   ],
   "id": "852d5247313debdf",
   "outputs": [],
   "execution_count": 1559
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T00:11:37.238960Z",
     "start_time": "2025-03-10T00:11:37.236342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "id": "2eba7d3a6391ac3c",
   "outputs": [],
   "execution_count": 1560
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T00:11:38.244236Z",
     "start_time": "2025-03-10T00:11:37.268142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import Dataset\n",
    "train = pd.read_csv(\"data/train.csv\")"
   ],
   "id": "ed5d4ed564e62c86",
   "outputs": [],
   "execution_count": 1561
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T00:11:38.729031Z",
     "start_time": "2025-03-10T00:11:38.248907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data Preprocessing\n",
    "data = np.array(train)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data)\n",
    "\n",
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape\n",
    "X_train = X_train.T"
   ],
   "id": "f1f0258415e2b31e",
   "outputs": [],
   "execution_count": 1562
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T00:11:39.467951Z",
     "start_time": "2025-03-10T00:11:38.743397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize Artificial Neural Network\n",
    "ann = ArtificialNeuralNetwork(input_size, output_size, hidden_layers, 128, InitializerType.RANDOM_DIST_NORMAL, InitializerType.RANDOM_DIST_NORMAL, 0, 0.01, 0, ActivationFunction.sigmoid)"
   ],
   "id": "fa6ea97943bbfc96",
   "outputs": [],
   "execution_count": 1563
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T00:11:40.637902Z",
     "start_time": "2025-03-10T00:11:39.483190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train Artificial Neural Network\n",
    "ann.train(torch.tensor(X_train), torch.tensor(Y_train), learning_rate, 100, LossFunction.mean_squared_error, verbose = True)"
   ],
   "id": "19e428bf85e6d3bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name: Input Layer\n",
      "Neurons: 128\n",
      "\n",
      "Layer Name: Hidden Layer 0\n",
      "Neurons: 128\n",
      "\n",
      "Layer Name: Output Layer\n",
      "Neurons: 10\n",
      "\n",
      "tensor(8) tensor([0.0077, 0.0077, 0.0078, 0.0078, 0.0079, 0.0079, 0.0078, 0.0076, 0.0078,\n",
      "        0.0077, 0.0078, 0.0078, 0.0078, 0.0079, 0.0079, 0.0078, 0.0077, 0.0077,\n",
      "        0.0079, 0.0079, 0.0079, 0.0077, 0.0078, 0.0080, 0.0079, 0.0078, 0.0078,\n",
      "        0.0078, 0.0079, 0.0079, 0.0079, 0.0077, 0.0078, 0.0078, 0.0078, 0.0078,\n",
      "        0.0078, 0.0078, 0.0079, 0.0078, 0.0078, 0.0079, 0.0078, 0.0078, 0.0078,\n",
      "        0.0080, 0.0077, 0.0077, 0.0078, 0.0079, 0.0079, 0.0078, 0.0078, 0.0078,\n",
      "        0.0079, 0.0078, 0.0078, 0.0079, 0.0079, 0.0081, 0.0077, 0.0077, 0.0080,\n",
      "        0.0078, 0.0078, 0.0079, 0.0079, 0.0079, 0.0077, 0.0080, 0.0078, 0.0078,\n",
      "        0.0077, 0.0078, 0.0078, 0.0078, 0.0077, 0.0079, 0.0078, 0.0079, 0.0078,\n",
      "        0.0079, 0.0077, 0.0078, 0.0079, 0.0078, 0.0079, 0.0078, 0.0078, 0.0080,\n",
      "        0.0077, 0.0078, 0.0077, 0.0077, 0.0078, 0.0079, 0.0079, 0.0077, 0.0078,\n",
      "        0.0078, 0.0076, 0.0077, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
      "        0.0078, 0.0077, 0.0079, 0.0077, 0.0078, 0.0076, 0.0078, 0.0078, 0.0079,\n",
      "        0.0078, 0.0077, 0.0078, 0.0078, 0.0078, 0.0080, 0.0078, 0.0078, 0.0079,\n",
      "        0.0079, 0.0079])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (784) must match the size of tensor b (128) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1564], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Train Artificial Neural Network\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mann\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mLossFunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean_squared_error\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[1558], line 51\u001B[0m, in \u001B[0;36mArtificialNeuralNetwork.train\u001B[1;34m(self, x, y, lr, epochs, loss_func, verbose)\u001B[0m\n\u001B[0;32m     49\u001B[0m     y_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(x_input)\n\u001B[0;32m     50\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_func(y_pred, y_input)\n\u001B[1;32m---> 51\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verbose:\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[1558], line 37\u001B[0m, in \u001B[0;36mArtificialNeuralNetwork.backward\u001B[1;34m(self, lr, output, target)\u001B[0m\n\u001B[0;32m     35\u001B[0m delta_w \u001B[38;5;241m=\u001B[39m lr \u001B[38;5;241m*\u001B[39m error_node \u001B[38;5;241m*\u001B[39m output\n\u001B[0;32m     36\u001B[0m delta_b \u001B[38;5;241m=\u001B[39m lr \u001B[38;5;241m*\u001B[39m error_node\n\u001B[1;32m---> 37\u001B[0m \u001B[43mlayer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight_update\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdelta_w\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m layer\u001B[38;5;241m.\u001B[39mbias_update(lr, delta_b)\n\u001B[0;32m     39\u001B[0m output \u001B[38;5;241m=\u001B[39m error_node\n",
      "Cell \u001B[1;32mIn[1555], line 22\u001B[0m, in \u001B[0;36mLayer.weight_update\u001B[1;34m(self, lr, grad)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mweight_update\u001B[39m(\u001B[38;5;28mself\u001B[39m, lr, grad):\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m neuron \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneurons:\n\u001B[1;32m---> 22\u001B[0m         \u001B[43mneuron\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight_update\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[1556], line 18\u001B[0m, in \u001B[0;36mNeuron.weight_update\u001B[1;34m(self, lr, grad)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mweight_update\u001B[39m(\u001B[38;5;28mself\u001B[39m, lr, grad):\n\u001B[1;32m---> 18\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweights \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m lr \u001B[38;5;241m*\u001B[39m grad\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (784) must match the size of tensor b (128) at non-singleton dimension 0"
     ]
    }
   ],
   "execution_count": 1564
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
