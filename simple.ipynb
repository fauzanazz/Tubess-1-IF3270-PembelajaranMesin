{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Scratch Artificial Neural Network",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T10:00:14.100968Z",
     "start_time": "2025-03-10T10:00:14.091920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import libraries\n",
    "# External libraries\n",
    "import torch\n",
    "\n",
    "# Python built-in libraries\n",
    "from functools import lru_cache\n",
    "from enum import Enum\n"
   ],
   "id": "7df9140457caed5e",
   "outputs": [],
   "execution_count": 1208
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T10:00:14.121306Z",
     "start_time": "2025-03-10T10:00:14.116440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Disable __grad based on assistant direction\n",
    "torch.set_grad_enabled(False)"
   ],
   "id": "5ccd2101c23e2da9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x323dec4d0>"
      ]
     },
     "execution_count": 1209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1209
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Classes for Artificial Neural Network",
   "id": "4be3dbc77bf6c823"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T10:00:14.149938Z",
     "start_time": "2025-03-10T10:00:14.145415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ActivationFunction:\n",
    "    \"\"\"\n",
    "    ActivationFunction Class.\n",
    "    \n",
    "    This class is used to define the activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=None)\n",
    "    def linear(x):\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=None)\n",
    "    def relu(x):\n",
    "        return torch.relu(x)\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=None)\n",
    "    def sigmoid(x):\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=None)\n",
    "    def tanh(x):\n",
    "        return torch.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    @lru_cache(maxsize=None)\n",
    "    def softmax(x):\n",
    "        return torch.softmax(x, dim=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_linear(_):\n",
    "        return 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_relu(x):\n",
    "        return torch.where(x > 0, 1, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_sigmoid(x):\n",
    "        return torch.sigmoid(x) * (1 - torch.sigmoid(x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def derivative_tanh(x):\n",
    "        return (2 / torch.exp(x) - torch.exp(-x)) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def derivative_softmax(x):\n",
    "        s = torch.softmax(x, dim=0).reshape(-1, 1)\n",
    "        return torch.diagflat(s) - torch.mm(s, s.T)"
   ],
   "id": "3102da3f17a7e376",
   "outputs": [],
   "execution_count": 1210
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T10:00:14.161627Z",
     "start_time": "2025-03-10T10:00:14.158608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LossFunction:\n",
    "    \"\"\"\n",
    "    LossFunction Class.\n",
    "    \n",
    "    This class is used to define the loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_squared_error(y_pred, y_true):\n",
    "        return torch.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def binary_cross_entropy(y_pred, y_true):\n",
    "        return -torch.sum(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "\n",
    "    @staticmethod\n",
    "    def categorical_cross_entropy(y_pred, y_true):\n",
    "        return -torch.sum(y_true * torch.log(y_pred))"
   ],
   "id": "f2d2791826ce2995",
   "outputs": [],
   "execution_count": 1211
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T10:00:14.171425Z",
     "start_time": "2025-03-10T10:00:14.169404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class InitializerType(Enum):\n",
    "    \"\"\"\n",
    "    InitializerType Enum.\n",
    "    \n",
    "    This enum is used to define the type of weight initialization.\n",
    "    Attributes:\n",
    "        ZERO: Zero initialization\n",
    "        RANDOM_DIST_UNIFORM: Random distribution uniform initialization\n",
    "        RANDOM_DIST_NORMAL: Random distribution normal initialization\n",
    "    \"\"\"\n",
    "    ZERO = 0\n",
    "    RANDOM_DIST_UNIFORM = 1\n",
    "    RANDOM_DIST_NORMAL = 2"
   ],
   "id": "829a7d3f4b16addc",
   "outputs": [],
   "execution_count": 1212
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T10:00:14.185304Z",
     "start_time": "2025-03-10T10:00:14.178570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Initializer:\n",
    "    \"\"\"\n",
    "    Initializer Class.\n",
    "    \n",
    "    This class is used to initialize weights and biases.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def init_weights(weight_init : InitializerType, param_1, param_2, size: int):\n",
    "        \"\"\"\n",
    "        Initialize weights\n",
    "        :param weight_init: Type of weight initialization\n",
    "        :param param_1: Lower bound or mean\n",
    "        :param param_2: Upper bound or standard deviation\n",
    "        :param size: Size of the weight\n",
    "        :return: Initialized weights\n",
    "        \"\"\"\n",
    "        if weight_init == InitializerType.ZERO:\n",
    "            return Initializer.zero_init(size)\n",
    "        elif weight_init == InitializerType.RANDOM_DIST_UNIFORM:\n",
    "            return Initializer.random_dist_uniform(size, param_1, param_2)\n",
    "        elif weight_init == InitializerType.RANDOM_DIST_NORMAL:\n",
    "            return Initializer.random_dist_normal(size, param_1, param_2)\n",
    "\n",
    "    @staticmethod\n",
    "    def init_bias(bias_init: InitializerType, param_1, param_2):\n",
    "        \"\"\"\n",
    "        Initialize Bias\n",
    "        :param bias_init: Type of bias initialization\n",
    "        :param param_1: Lower bound or mean\n",
    "        :param param_2: Upper bound or standard deviation\n",
    "        :return: Initialized bias\n",
    "        \"\"\"\n",
    "        if bias_init == InitializerType.ZERO:\n",
    "            return 0\n",
    "        elif bias_init == InitializerType.RANDOM_DIST_UNIFORM:\n",
    "            return torch.rand(1) * (param_2 - param_1) + param_1\n",
    "        elif bias_init == InitializerType.RANDOM_DIST_NORMAL:\n",
    "            return torch.randn(1) * param_2 + param_1\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def zero_init(size):\n",
    "        return torch.zeros(size)\n",
    "\n",
    "    @staticmethod\n",
    "    def random_dist_uniform(size, lower_bound, upper_bound):\n",
    "        return torch.rand(size) * (upper_bound - lower_bound) + lower_bound\n",
    "\n",
    "    @staticmethod\n",
    "    def random_dist_normal(size, mean, std):\n",
    "        return torch.randn(size) * std + mean"
   ],
   "id": "1013ba92464b7584",
   "outputs": [],
   "execution_count": 1213
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-10T10:00:14.195836Z",
     "start_time": "2025-03-10T10:00:14.193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Neuron:\n",
    "    \"\"\"\n",
    "    Neuron Class.\n",
    "    \n",
    "    Consist of weights and bias for every input feature.\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_init : InitializerType, bias_init : InitializerType, input_size, param_1, param_2):\n",
    "        self.weights = Initializer.init_weights(weight_init, param_1, param_2, input_size)\n",
    "        self.bias = Initializer.init_bias(bias_init, param_1, param_2)\n",
    "        self.cost_weight = torch.zeros(input_size)\n",
    "        self.cost_bias = torch.zeros(input_size)\n",
    "        self.error_node = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sum(x * self.weights) + self.bias\n",
    "\n",
    "    def weight_update(self):\n",
    "        self.weights += self.cost_weight\n",
    "        \n",
    "    def bias_update(self):\n",
    "        self.bias += self.cost_bias"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1214
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T10:00:14.208015Z",
     "start_time": "2025-03-10T10:00:14.203583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Utils:\n",
    "    \"\"\"\n",
    "    Encoder Class\n",
    "\n",
    "    Encode values\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def output_minus_target(output, target):\n",
    "        \"\"\"\n",
    "        Calculate output minus target\n",
    "        :param output: Tensor Array of output\n",
    "        :param target: Int target\n",
    "        :return: Tensor Array of output minus target\n",
    "        \"\"\"\n",
    "        target_array = torch.zeros_like(output)\n",
    "        target_array[target] = 1\n",
    "        return output - target_array"
   ],
   "id": "24e2fecc7f33942d",
   "outputs": [],
   "execution_count": 1215
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T10:00:14.221039Z",
     "start_time": "2025-03-10T10:00:14.216037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Layer Class.\n",
    "\n",
    "    Consist of many neurons of Type Layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight_init : InitializerType, bias_init : InitializerType, input_size, output_size, param_1, param_2, activation = ActivationFunction.linear, layer_name = None):\n",
    "        self.layer_name = layer_name\n",
    "        self.neurons = [Neuron(weight_init, bias_init, input_size, param_1, param_2) for _ in range(output_size)]\n",
    "        self.activation_func = activation\n",
    "        self.sum = None\n",
    "        self.output = None\n",
    "        self.derivative_activation = None\n",
    "        self.error_node = None\n",
    "\n",
    "        match activation:\n",
    "            case ActivationFunction.linear:\n",
    "                self.derivative_activation = ActivationFunction.derivative_linear\n",
    "            case ActivationFunction.relu:\n",
    "                self.derivative_activation = ActivationFunction.derivative_relu\n",
    "            case ActivationFunction.sigmoid:\n",
    "                self.derivative_activation = ActivationFunction.derivative_sigmoid\n",
    "            case ActivationFunction.tanh:\n",
    "                self.derivative_activation = ActivationFunction.derivative_tanh\n",
    "            case ActivationFunction.softmax:\n",
    "                self.derivative_activation = ActivationFunction.derivative_softmax\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.sum = torch.stack([neuron.forward(x) for neuron in self.neurons])\n",
    "        self.output = self.activation_func(self.sum)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, lr, prev_layer, target=None):\n",
    "        # Iterate through all neurons\n",
    "        self.error_node = torch.zeros_like(self.output)\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            # Calculate error node\n",
    "            sum_of_weight = torch.sum(neuron.weights * prev_layer.error_node)\n",
    "            error_node = self.output[i] * (1 - self.output[i]) * sum_of_weight\n",
    "            neuron.error_node = error_node\n",
    "            self.error_node[i] = error_node\n",
    "            # Calculate cost weight\n",
    "            self.neurons[i].cost_weight = -lr * error_node * self.output[i]\n",
    "            # Calculate cost bias\n",
    "            self.neurons[i].cost_bias = -lr * error_node * self.neurons[i].bias\n",
    "\n",
    "    def update_weight(self):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.weight_update()\n",
    "\n",
    "    def update_bias(self):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.bias_update()\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Layer Name: {self.layer_name}\\nNeurons: {len(self.neurons)}\\n\""
   ],
   "id": "e47d0abebcf3846d",
   "outputs": [],
   "execution_count": 1216
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T10:00:14.237562Z",
     "start_time": "2025-03-10T10:00:14.234574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class OutputLayer(Layer):\n",
    "    \"\"\"\n",
    "    OutputLayer Class.\n",
    "    \n",
    "    Consist of many neurons of Output Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_init : InitializerType, bias_init : InitializerType, input_size, output_size, param_1, param_2, activation = ActivationFunction.linear, layer_name = None):\n",
    "        super().__init__(weight_init, bias_init, input_size, output_size, param_1, param_2, activation, layer_name)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.sum = torch.stack([neuron.forward(x) for neuron in self.neurons])\n",
    "        self.output = ActivationFunction.sigmoid(self.sum)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, lr, target = None, layer = None):\n",
    "        self.error_node = torch.zeros_like(self.output)\n",
    "        target_min_output = Utils.output_minus_target(self.output, target)\n",
    "        for i, neuron in enumerate(self.neurons):\n",
    "            target_delta = target_min_output[i]\n",
    "            error_node = self.output[i] * (1 - self.output[i]) * target_delta\n",
    "            self.error_node[i] = error_node\n",
    "            neuron.error_node = error_node\n",
    "            self.neurons[i].cost_weight = -lr * error_node * self.output[i]\n",
    "            self.neurons[i].cost_bias = -lr * error_node\n",
    "\n"
   ],
   "id": "808b47ccd483552c",
   "outputs": [],
   "execution_count": 1217
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T10:00:14.247777Z",
     "start_time": "2025-03-10T10:00:14.245675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class InputLayer:\n",
    "    \"\"\"\n",
    "    InputLayer Class.\n",
    "\n",
    "    Consist of many neurons of Input Layer\n",
    "    This layer only store input without weight.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, layer_name = None):\n",
    "        self.input_size = input_size\n",
    "        self.input = torch.zeros(input_size)\n",
    "        self.layer_name = layer_name\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return x\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Layer Name: {self.layer_name}\\nNeurons: {self.input_size}\\n\""
   ],
   "id": "f05458f95cc9d8ef",
   "outputs": [],
   "execution_count": 1218
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T10:00:14.264477Z",
     "start_time": "2025-03-10T10:00:14.257045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ArtificialNeuralNetwork:\n",
    "    \"\"\"\n",
    "    ArtificialNeuralNetwork Class.\n",
    "    \n",
    "    Consist of many layers of Type Layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, hidden_layers, hidden_size, weight_init, bias_init, param_1, param_2, seeds = 0, activation = ActivationFunction.linear, loss_func = LossFunction.mean_squared_error):\n",
    "        self.layers = []\n",
    "        self.layers.append(InputLayer(input_size, \"Input Layer\"))\n",
    "        for i in range(hidden_layers):\n",
    "            if i == 0:\n",
    "                self.layers.append(Layer(weight_init, bias_init, input_size, hidden_size, param_1, param_2, activation, f\"Hidden Layer {i}\"))\n",
    "            else:\n",
    "                self.layers.append(Layer(weight_init, bias_init, hidden_size, hidden_size, param_1, param_2, activation, f\"Hidden Layer {i}\"))\n",
    "        self.layers.append(OutputLayer(weight_init, bias_init, hidden_size, output_size, param_1, param_2, activation, \"Output Layer\"))\n",
    "        self.loss_func = loss_func\n",
    "        torch.manual_seed(seeds)\n",
    "        self.target = None\n",
    "        self.target_min_output = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, lr, target):\n",
    "        index = len(self.layers) - 1\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, InputLayer):\n",
    "                for layer_up in self.layers:\n",
    "                    if isinstance(layer_up, InputLayer):\n",
    "                        continue\n",
    "                    layer_up.update_bias()\n",
    "                    layer_up.update_weight()\n",
    "            else:\n",
    "                if isinstance(layer, OutputLayer):\n",
    "                    layer.backward(lr, target)\n",
    "                else:\n",
    "                    layer.backward(lr, self.layers[index], target)\n",
    "            index -= 1\n",
    "\n",
    "\n",
    "    def train(self, x, y, lr, epochs, loss_func, verbose = False):\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            iter = 1\n",
    "            for x_input, y_input in zip(x, y):\n",
    "                y_pred = self.forward(x_input)\n",
    "                loss = loss_func(y_pred, y_input)\n",
    "                self.target_min_output = Utils.output_minus_target(y_pred, y_input)\n",
    "                self.backward(lr, y_input)\n",
    "                if iter % 1000 == 0 and verbose:\n",
    "                    print(f\"Iter {iter} - Loss: {loss}\")\n",
    "                iter += 1\n",
    "            if verbose:\n",
    "                print(f\"Epoch {epoch} - Loss: {loss}\")"
   ],
   "id": "186cab437e129e46",
   "outputs": [],
   "execution_count": 1219
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pipeline for testing MNIST dataset",
   "id": "6c34fd47c03a7a52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T10:00:14.278245Z",
     "start_time": "2025-03-10T10:00:14.274723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Const variables\n",
    "input_size = 784\n",
    "hidden_layers = 1\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "param_1 = 1\n",
    "param_2 = 1"
   ],
   "id": "852d5247313debdf",
   "outputs": [],
   "execution_count": 1220
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T10:00:14.295205Z",
     "start_time": "2025-03-10T10:00:14.292764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "id": "2eba7d3a6391ac3c",
   "outputs": [],
   "execution_count": 1221
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T10:00:15.893439Z",
     "start_time": "2025-03-10T10:00:14.297337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import Dataset\n",
    "train = pd.read_csv(\"data/train.csv\")"
   ],
   "id": "ed5d4ed564e62c86",
   "outputs": [],
   "execution_count": 1222
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T10:00:16.730006Z",
     "start_time": "2025-03-10T10:00:15.927968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Data Preprocessing\n",
    "data = np.array(train)\n",
    "m, n = data.shape\n",
    "np.random.shuffle(data)\n",
    "\n",
    "data_dev = data[0:1000].T\n",
    "Y_dev = data_dev[0]\n",
    "X_dev = data_dev[1:n]\n",
    "X_dev = X_dev / 255.\n",
    "\n",
    "data_train = data[1000:m].T\n",
    "Y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255.\n",
    "_,m_train = X_train.shape\n",
    "X_train = X_train.T"
   ],
   "id": "f1f0258415e2b31e",
   "outputs": [],
   "execution_count": 1223
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T10:00:16.796244Z",
     "start_time": "2025-03-10T10:00:16.772204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize Artificial Neural Network\n",
    "ann = ArtificialNeuralNetwork(input_size, output_size, hidden_layers, 128, InitializerType.RANDOM_DIST_NORMAL, InitializerType.RANDOM_DIST_NORMAL, param_1, param_2, 0, ActivationFunction.sigmoid, LossFunction.mean_squared_error)"
   ],
   "id": "fa6ea97943bbfc96",
   "outputs": [],
   "execution_count": 1224
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T10:14:35.167205Z",
     "start_time": "2025-03-10T10:00:16.812757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train Artificial Neural Network\n",
    "ann.train(torch.tensor(X_train), torch.tensor(Y_train), learning_rate, 100, LossFunction.mean_squared_error, verbose = True)"
   ],
   "id": "19e428bf85e6d3bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1000 - Loss: 1.0\n",
      "Iter 2000 - Loss: 4.0\n",
      "Iter 3000 - Loss: 0.0\n",
      "Iter 4000 - Loss: 0.0\n",
      "Iter 5000 - Loss: 1.0\n",
      "Iter 6000 - Loss: 64.0\n",
      "Iter 7000 - Loss: 64.0\n",
      "Iter 8000 - Loss: 1.0\n",
      "Iter 9000 - Loss: 0.0\n",
      "Iter 10000 - Loss: 49.0\n",
      "Iter 11000 - Loss: 64.0\n",
      "Iter 12000 - Loss: 49.0\n",
      "Iter 13000 - Loss: 1.0\n",
      "Iter 14000 - Loss: 0.0\n",
      "Iter 15000 - Loss: 0.0\n",
      "Iter 16000 - Loss: 49.0\n",
      "Iter 17000 - Loss: 1.0\n",
      "Iter 18000 - Loss: 25.0\n",
      "Iter 19000 - Loss: 64.0\n",
      "Iter 20000 - Loss: 16.0\n",
      "Iter 21000 - Loss: 36.0\n",
      "Iter 22000 - Loss: 49.0\n",
      "Iter 23000 - Loss: 16.0\n",
      "Iter 24000 - Loss: 25.0\n",
      "Iter 25000 - Loss: 49.0\n",
      "Iter 26000 - Loss: 1.0\n",
      "Iter 27000 - Loss: 25.0\n",
      "Iter 28000 - Loss: 9.0\n",
      "Iter 29000 - Loss: 4.0\n",
      "Iter 30000 - Loss: 9.0\n",
      "Iter 31000 - Loss: 36.0\n",
      "Iter 32000 - Loss: 25.0\n",
      "Iter 33000 - Loss: 1.0\n",
      "Iter 34000 - Loss: 64.0\n",
      "Iter 35000 - Loss: 25.0\n",
      "Iter 36000 - Loss: 0.0\n",
      "Iter 37000 - Loss: 9.0\n",
      "Iter 38000 - Loss: 4.0\n",
      "Iter 39000 - Loss: 25.0\n",
      "Iter 40000 - Loss: 1.0\n",
      "Iter 41000 - Loss: 9.0\n",
      "Epoch 0 - Loss: 9.0\n",
      "Iter 1000 - Loss: 1.0\n",
      "Iter 2000 - Loss: 4.0\n",
      "Iter 3000 - Loss: 0.0\n",
      "Iter 4000 - Loss: 0.0\n",
      "Iter 5000 - Loss: 1.0\n",
      "Iter 6000 - Loss: 64.0\n",
      "Iter 7000 - Loss: 64.0\n",
      "Iter 8000 - Loss: 1.0\n",
      "Iter 9000 - Loss: 0.0\n",
      "Iter 10000 - Loss: 49.0\n",
      "Iter 11000 - Loss: 64.0\n",
      "Iter 12000 - Loss: 49.0\n",
      "Iter 13000 - Loss: 1.0\n",
      "Iter 14000 - Loss: 0.0\n",
      "Iter 15000 - Loss: 0.0\n",
      "Iter 16000 - Loss: 49.0\n",
      "Iter 17000 - Loss: 1.0\n",
      "Iter 18000 - Loss: 25.0\n",
      "Iter 19000 - Loss: 64.0\n",
      "Iter 20000 - Loss: 16.0\n",
      "Iter 21000 - Loss: 36.0\n",
      "Iter 22000 - Loss: 49.0\n",
      "Iter 23000 - Loss: 16.0\n",
      "Iter 24000 - Loss: 25.0\n",
      "Iter 25000 - Loss: 49.0\n",
      "Iter 26000 - Loss: 1.0\n",
      "Iter 27000 - Loss: 25.0\n",
      "Iter 28000 - Loss: 9.0\n",
      "Iter 29000 - Loss: 4.0\n",
      "Iter 30000 - Loss: 9.0\n",
      "Iter 31000 - Loss: 36.0\n",
      "Iter 32000 - Loss: 25.0\n",
      "Iter 33000 - Loss: 1.0\n",
      "Iter 34000 - Loss: 64.0\n",
      "Iter 35000 - Loss: 25.0\n",
      "Iter 36000 - Loss: 0.0\n",
      "Iter 37000 - Loss: 9.0\n",
      "Iter 38000 - Loss: 4.0\n",
      "Iter 39000 - Loss: 25.0\n",
      "Iter 40000 - Loss: 1.0\n",
      "Iter 41000 - Loss: 9.0\n",
      "Epoch 1 - Loss: 9.0\n",
      "Iter 1000 - Loss: 1.0\n",
      "Iter 2000 - Loss: 4.0\n",
      "Iter 3000 - Loss: 0.0\n",
      "Iter 4000 - Loss: 0.0\n",
      "Iter 5000 - Loss: 1.0\n",
      "Iter 6000 - Loss: 64.0\n",
      "Iter 7000 - Loss: 64.0\n",
      "Iter 8000 - Loss: 1.0\n",
      "Iter 9000 - Loss: 0.0\n",
      "Iter 10000 - Loss: 49.0\n",
      "Iter 11000 - Loss: 64.0\n",
      "Iter 12000 - Loss: 49.0\n",
      "Iter 13000 - Loss: 1.0\n",
      "Iter 14000 - Loss: 0.0\n",
      "Iter 15000 - Loss: 0.0\n",
      "Iter 16000 - Loss: 49.0\n",
      "Iter 17000 - Loss: 1.0\n",
      "Iter 18000 - Loss: 25.0\n",
      "Iter 19000 - Loss: 64.0\n",
      "Iter 20000 - Loss: 16.0\n",
      "Iter 21000 - Loss: 36.0\n",
      "Iter 22000 - Loss: 49.0\n",
      "Iter 23000 - Loss: 16.0\n",
      "Iter 24000 - Loss: 25.0\n",
      "Iter 25000 - Loss: 49.0\n",
      "Iter 26000 - Loss: 1.0\n",
      "Iter 27000 - Loss: 25.0\n",
      "Iter 28000 - Loss: 9.0\n",
      "Iter 29000 - Loss: 4.0\n",
      "Iter 30000 - Loss: 9.0\n",
      "Iter 31000 - Loss: 36.0\n",
      "Iter 32000 - Loss: 25.0\n",
      "Iter 33000 - Loss: 1.0\n",
      "Iter 34000 - Loss: 64.0\n",
      "Iter 35000 - Loss: 25.0\n",
      "Iter 36000 - Loss: 0.0\n",
      "Iter 37000 - Loss: 9.0\n",
      "Iter 38000 - Loss: 4.0\n",
      "Iter 39000 - Loss: 25.0\n",
      "Iter 40000 - Loss: 1.0\n",
      "Iter 41000 - Loss: 9.0\n",
      "Epoch 2 - Loss: 9.0\n",
      "Iter 1000 - Loss: 1.0\n",
      "Iter 2000 - Loss: 4.0\n",
      "Iter 3000 - Loss: 0.0\n",
      "Iter 4000 - Loss: 0.0\n",
      "Iter 5000 - Loss: 1.0\n",
      "Iter 6000 - Loss: 64.0\n",
      "Iter 7000 - Loss: 64.0\n",
      "Iter 8000 - Loss: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1225], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Train Artificial Neural Network\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m ann\u001B[38;5;241m.\u001B[39mtrain(torch\u001B[38;5;241m.\u001B[39mtensor(X_train), torch\u001B[38;5;241m.\u001B[39mtensor(Y_train), learning_rate, \u001B[38;5;241m100\u001B[39m, LossFunction\u001B[38;5;241m.\u001B[39mmean_squared_error, verbose \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[1219], line 51\u001B[0m, in \u001B[0;36mArtificialNeuralNetwork.train\u001B[0;34m(self, x, y, lr, epochs, loss_func, verbose)\u001B[0m\n\u001B[1;32m     49\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_func(y_pred, y_input)\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_min_output \u001B[38;5;241m=\u001B[39m Utils\u001B[38;5;241m.\u001B[39moutput_minus_target(y_pred, y_input)\n\u001B[0;32m---> 51\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackward(lr, y_input)\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28miter\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m1000\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m verbose:\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIter \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28miter\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[1219], line 39\u001B[0m, in \u001B[0;36mArtificialNeuralNetwork.backward\u001B[0;34m(self, lr, target)\u001B[0m\n\u001B[1;32m     37\u001B[0m         layer\u001B[38;5;241m.\u001B[39mbackward(lr, target)\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 39\u001B[0m         layer\u001B[38;5;241m.\u001B[39mbackward(lr, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[index], target)\n\u001B[1;32m     40\u001B[0m index \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Cell \u001B[0;32mIn[1216], line 39\u001B[0m, in \u001B[0;36mLayer.backward\u001B[0;34m(self, lr, prev_layer, target)\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror_node \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros_like(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput)\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, neuron \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneurons):\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;66;03m# Calculate error node\u001B[39;00m\n\u001B[0;32m---> 39\u001B[0m     sum_of_weight \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(neuron\u001B[38;5;241m.\u001B[39mweights \u001B[38;5;241m*\u001B[39m prev_layer\u001B[38;5;241m.\u001B[39merror_node)\n\u001B[1;32m     40\u001B[0m     error_node \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput[i] \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput[i]) \u001B[38;5;241m*\u001B[39m sum_of_weight\n\u001B[1;32m     41\u001B[0m     neuron\u001B[38;5;241m.\u001B[39merror_node \u001B[38;5;241m=\u001B[39m error_node\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1225
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
